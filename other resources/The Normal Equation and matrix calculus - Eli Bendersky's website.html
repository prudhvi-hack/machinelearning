<!DOCTYPE html>
<!-- saved from url=(0075)https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:og="http://ogp.me/ns#" xmlns:fb="https://www.facebook.com/2008/fbml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>The Normal Equation and matrix calculus - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="https://eli.thegreenplace.net/favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/bootstrap.min.css" type="text/css">
    <link href="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/font-awesome.min.css" rel="stylesheet">

    <link href="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/style.css" type="text/css">

        <link href="https://eli.thegreenplace.net/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Eli Bendersky&#39;s website ATOM Feed">

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://eli.thegreenplace.net/" class="navbar-brand">
                <img src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/logosmall.png" width="32">
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="https://eli.thegreenplace.net/pages/about">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="https://eli.thegreenplace.net/archives/all">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/" rel="bookmark" title="Permalink to The Normal Equation and matrix calculus">
                        The Normal Equation and matrix calculus
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> May 27, 2015 at 06:19</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="https://eli.thegreenplace.net/tag/math">Math</a>
        ,
    <a href="https://eli.thegreenplace.net/tag/machine-learning">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>A few months ago I wrote <a class="reference external" href="http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression">a post</a>
on formulating the Normal Equation for linear regression. A crucial part in the
formulation is using <a class="reference external" href="http://en.wikipedia.org/wiki/Matrix_calculus">matrix calculus</a> to compute a scalar-by-vector
derivative. I didn't spend much time explaining how this step works, instead
remarking:</p>
<blockquote>
Deriving by a vector may feel uncomfortable, but there's nothing to worry
about. Recall that here we only use matrix notation to conveniently
represent a system of linear formulae. So we derive by each component of the
vector, and then combine the resulting derivatives into a vector again.</blockquote>
<p>According to the comments received on the post, folks didn't find this
convincing and asked for more details. One commenter even said that "matrix
calculus feels handwavy", something which I fully agree with. The reason matrix
calculus feels handwavy is that it's not as commonly encountered as "regular"
calculus, and hence its identities and intuitions are not as familiar. However,
there's really not that much to it, as I want to show here.</p>
<p>Let's get started with a simple example, which I'll use to demonstrate the
principles. Say we have the function:</p>
<img alt="\[f(v)=a^Tv\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/94f87149715376908db65a00f793836a4b2092a9.png" style="height: 21px;">
<p>Where <strong>a</strong> and <strong>v</strong> are vectors with <em>n</em> components <a class="footnote-reference" href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id4" id="id1">[1]</a>. We want to compute
its derivative by <strong>v</strong>. But wait, while a "regular" derivative by a scalar is
clearly defined (using limits), what does deriving by a vector mean? It simply
means that we derive by each component of the vector separately, and then
combine the results into a new vector <a class="footnote-reference" href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id5" id="id2">[2]</a>. In other words:</p>
<img alt="\[\frac{\partial f}{\partial v}=\begin{pmatrix}\frac{\partial f}{\partial v_1}\\[1em] \frac{\partial f}{\partial v_2}\\ ...\\ \frac{\partial f}{\partial v_n}\\[1em] \end{pmatrix}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/13d227107c5323f47460ad077504fda60726d933.png" style="height: 131px;">
<p>Let's see how this works out for our function <em>f</em>. It may be more convenient to
rewrite it by using components rather than vector notation:</p>
<img alt="\[f(v)=a^Tv=a_1v_1+a_2v_2+...+a_nv_n\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/e9e17e44bb85d825f304b09247a7f3cfbe11f64e.png" style="height: 21px;">
<p>Computing the derivatives by each component, we'll get:</p>
<img alt="\[\begin{matrix}\frac{\partial f}{\partial v_1}=a_1\\[1em] \frac{\partial f}{\partial v_2}=a_2\\ ...\\ \frac{\partial f}{\partial v_n}=a_n\\[1em] \end{matrix}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/768563e5b7e2e8cddd00830e9b945419f598e4bb.png" style="height: 114px;">
<p>So we have a sequence of partial derivatives, which we combine into a vector:</p>
<img alt="\[\frac{\partial f}{\partial v}=\begin{pmatrix}a_1\\ ...\\ a_n\\ \end{pmatrix}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/b13cc64568603d73240709c1fb49cfcc7f2a2b62.png" style="height: 65px;">
<p>Or, in other words <img alt="\frac{\partial f}{\partial v}=a" class="valign-m7" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/1f3eaea99f7fab11ac1b70dc8b618635a9ed4c91.png" style="height: 25px;">.</p>
<p>This example demonstrates the algorithm for computing scalar-by-vector
derivatives:</p>
<ol class="arabic simple">
<li>Figure out what the dimensions of all vectors and matrices are.</li>
<li>Expand the vector equations into their full form (a multiplication of two
vectors is either a scalar or a matrix, depending on their orientation, etc.)
Note that this will end up with a scalar.</li>
<li>Compute the derivative of the scalar by each component of the variable vector
separately.</li>
<li>Combine the derivatives into a vector.</li>
</ol>
<p>Similarly to regular calculus, matrix and vector calculus rely on a set of
identities to make computations more manageable. We can either go the hard way
(computing the derivative of each function from basic principles using limits),
or the easy way - applying the plethora of convenient identities that were
developed to make this task simpler. The identity for computing the derivative
of <img alt="a^Tv" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/ea7bffcd29c6bad40e358ad7313102670fb1a9cf.png" style="height: 15px;"> shown above plays the role of <img alt="\frac{d}{dx}ax=a" class="valign-m6" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/999f262480b3690892d0af5651b96160d924997e.png" style="height: 22px;"> in
regular calculus.</p>
<p>Now we have the tools to understand how the vector derivatives in the
<a class="reference external" href="http://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression">normal equation article</a>
were computed. As a reminder, this is the matrix form of the cost function <em>J</em>:</p>
<img alt="\[J(\theta)=\theta^TX^TX\theta-2(X\theta)^Ty+y^Ty\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/2864b88546c007a79dc92271f5e01487ba608e43.png" style="height: 21px;">
<p>And we're interested in computing <img alt="\frac{\partial J}{\partial \theta}" class="valign-m7" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/27ffac3eede7fce0b342abf8fc10d29f24c68263.png" style="height: 24px;">.
The equation for <em>J</em> consists of three terms added together. The last one
<img alt="y^Ty" class="valign-m4" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/81015d6225923cec985bef47ca151ef1cb654c92.png" style="height: 19px;"> doesn't contribute to the derivative because it doesn't depend on
the variable. Let's start looking at the second (since it's simpler than the
first) - and give it a name, for convenience:</p>
<img alt="\[P(\theta)=2(X\theta)^Ty\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/35d3ddf05898e8bc2085030aa399ce98318674f9.png" style="height: 21px;">
<p>We'll start by recalling what all the dimensions are. <img alt="\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> is a vector
of n components. <img alt="y" class="valign-m4" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/95cb0bfd2977c761298d9624e4b4d4c72a39974a.png" style="height: 12px;"> is a vector of m components. <img alt="X" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/c032adc1ff629c9b66f22749ad667e6beadf144b.png" style="height: 12px;"> is a m-by-n
matrix.</p>
<p>Let's see what <em>P</em> expands to <a class="footnote-reference" href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id6" id="id3">[3]</a>:</p>
<img alt="\[P(\theta)=2\left [ \begin{pmatrix} x_1_1 &amp;amp; x_1_2 &amp;amp; ... &amp;amp; x_1_n\\ x_2_1 &amp;amp; ... &amp;amp; ... &amp;amp; x_2_n\\ ...\\ x_m_1 &amp;amp; ... &amp;amp; ... &amp;amp; x_m_n\\ \end{pmatrix}\begin{pmatrix} \theta_1\\ \theta_2\\ ...\\ \theta_n\\ \end{pmatrix} \right ]^T\begin{pmatrix} y_1\\ y_2\\ ...\\ y_m\\ \end{pmatrix}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/a7873ed04e274b30852e0f8d9450b5abc200ac17.png" style="height: 91px;">
<p>Computing the matrix-by-vector multiplication inside the parens:</p>
<img alt="\[P(x)=2\left [ \begin{pmatrix} x_1_1\theta_1+...+x_1_n\theta_n\\ x_2_1\theta_1+...+x_2_n\theta_n\\ ...\\ x_m_1\theta_1+...+x_m_n\theta_n \end{pmatrix} \right ]^T\begin{pmatrix} y_1\\ y_2\\ ...\\ y_m\\ \end{pmatrix}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/6b9b8e2335579f352a19ef3da609be2e8b2d9925.png" style="height: 91px;">
<p>And finally, multiplying the two vectors together:</p>
<img alt="\[P(x)=2(x_1_1\theta_1+...+x_1_n\theta_n)y_1+2(x_2_1\theta_1+...+x_2_n\theta_n)y_2+...+2(x_m_1\theta_1+...+x_m_n\theta_n)y_m\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/3271758ac98b149969516dd809fd35b90aacf056.png" style="height: 18px;">
<p>Working with such formulae makes you appreciate why mathematicians have long
ago come up with shorthand notations like "sigma" summation:</p>
<img alt="\[P(x)=2\sum_{r=1}^{m}y_r(x_r_1\theta_1+...+x_r_n\theta_n)=2\sum_{r=1}^{m}y_r\sum_{c=1}^{n}x_r_c\theta_c\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/6c71eb575ab3fafbc7b268be33d0d17a37bb1553.png" style="height: 50px;">
<p>OK, so we've finally completed step 2 of the algorithm - we have the scalar
equation for <em>P</em>. Now it's time to compute its derivative by each
<img alt="\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;">:</p>
<img alt="\[\begin{matrix} \frac{\partial P}{\partial \theta_1}=2(x_1_1y_1+...+x_m_1y_m)\\[1em] \frac{\partial P}{\partial \theta_2}=2(x_1_2y_1+...+x_m_2y_m)\\ ...\\ \frac{\partial P}{\partial \theta_n}=2(x_1_ny_1+...+x_m_ny_m) \end{matrix}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/889eb3c4e50b4fbdf5380c4d4e31ac4c0c09dddd.png" style="height: 111px;">
<p>Now comes the most interesting part. If we treat
<img alt="\frac{\partial P}{\partial \theta}" class="valign-m7" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/3c653fa292156c8914f1463fcb6869633d37487c.png" style="height: 24px;"> as a vector of n components, we can
rewrite this set of equations using a matrix-by-vector multiplication:</p>
<img alt="\[\frac{\partial P}{\partial \theta}=2X^Ty\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/7f75aa0f038ca73c58e95ef604ffb54468a18ae2.png" style="height: 38px;">
<p>Take a moment to convince yourself this is true. It's just collecting the
individual components of <strong>X</strong> into a matrix and the individual components of
<strong>y</strong> into a vector. Since <strong>X</strong> is a m-by-n matrix and <strong>y</strong> is a m-by-1 column
vector, the dimensions work out and the result is a n-by-1 column vector.</p>
<p>So we've just computed the second term of the vector derivative of <em>J</em>. In the
process, we've discovered a useful vector derivative identity for a matrix <strong>X</strong>
and vectors <img alt="\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> and <strong>y</strong>:</p>
<img alt="\[\frac{\partial (X\theta)^T y}{\partial \theta}=X^Ty\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/bf7325787bc464f067372a6d4ed612ea514d29b6.png" style="height: 41px;">
<p>OK, now let's get back to the full definition of <em>J</em> and see how to compute the
derivative of its first term. We'll give it the name <em>Q</em>:</p>
<img alt="\[Q(\theta)=\theta^TX^TX\theta\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/0031acbab8dba6cef63f2605a15a0b7bc826766a.png" style="height: 21px;">
<p>This derivation is somewhat more complex, since <img alt="\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> appears twice in
the equation. Here's the equation again with all the matrices and vectors fully
laid out (note that I've already done the transposes):</p>
<img alt="\[Q(\theta)=(\theta_1...\theta_n)\begin{pmatrix}x_1_1 &amp;amp; x_2_1 &amp;amp; ... &amp;amp; x_m_1\\ x_1_2 &amp;amp; ... &amp;amp; ... &amp;amp; x_m_2\\ ...\\ x_1_n &amp;amp; ... &amp;amp; ... &amp;amp; x_m_n\\ \end{pmatrix}\begin{pmatrix}x_1_1 &amp;amp; x_1_2 &amp;amp; ... &amp;amp; x_1_n\\ x_2_1 &amp;amp; ... &amp;amp; ... &amp;amp; x_2_n\\ ...\\ x_m_1 &amp;amp; ... &amp;amp; ... &amp;amp; x_m_n\\ \end{pmatrix}\begin{pmatrix} \theta_1\\ \theta_2\\ ...\\ \theta_n\\ \end{pmatrix}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/b3f9b4ffe1853d6610f9814fc820d1a71825a06e.png" style="height: 87px;">
<p>I'll just multiply the two matrices in the middle together. The result is a
"<strong>X</strong> squared" matrix, which is n-by-n. The element in row <em>r</em> and column <em>c</em>
of this square matrix is:</p>
<img alt="\[\sum_{i=1}^{m}x_i_rx_i_c\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/f8628d68855e03195fb4fd01806c8655beaf7b30.png" style="height: 50px;">
<p>Note that "<strong>X</strong> squared" is a symmetric matrix (this fact will be important
later on). For simplicity of notation, we'll call its elements
<img alt="X^{2}_{rc}" class="valign-m4" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/c565201908a5c75f62849e7c1634b65e0930824c.png" style="height: 19px;">. Multiplying by the <img alt="\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> vector on the right we
get:</p>
<img alt="\[Q(\theta)=(\theta_1...\theta_n)\begin{pmatrix}X^{2}_{11}\theta_1+...+X^{2}_{1n}\theta_n\\[1em] X^{2}_{21}\theta_1+...+X^{2}_{2n}\theta_n\\ ...\\ X^{2}_{n1}\theta_1+...+X^{2}_{nn}\theta_n\end{pmatrix}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/5821cf256f6cf6debbdac48d6e9bbe698baa0a11.png" style="height: 107px;">
<p>And left-multiplying by <img alt="\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> to get the fully unwrapped formula for
<em>Q</em>:</p>
<img alt="\[Q(\theta)=\theta_1(X^{2}_{11}\theta_1+...+X^{2}_{1n}\theta_n)+\theta_2(X^{2}_{21}\theta_1+...+X^{2}_{2n}\theta_n)+...+\theta_n(X^{2}_{n1}\theta_1+...+X^{2}_{nn}\theta_n)\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/0451f9fa7c61ff3a61be8c1836c15667cd916330.png" style="height: 22px;">
<p>Once again, it's now time to compute the derivatives. Let's focus on
<img alt="\frac{\partial Q}{\partial \theta_1}" class="valign-m9" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/5161830b1f644a3c2d1a650ccd6405e0fe5940aa.png" style="height: 27px;">, from which we can infer the rest:</p>
<img alt="\[\frac{\partial Q}{\partial \theta_1}=(2\theta_1X^{2}_{11}+\theta_2X^{2}_{12}+...+\theta_nX^{2}_{1n})+\theta_2X^{2}_{21}+...+\theta_nX^{2}_{n1}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/f99e5e7024b4d13b0a767b98653b6ccc22fa1abd.png" style="height: 41px;">
<p>Using the fact that <strong>X</strong> squared is symmetric, we know that
<img alt="X^{2}_{12}=X^{2}_{21}" class="valign-m6" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/c14595d1000ad9a8da5be7f37da801eadfdfb698.png" style="height: 21px;"> and so on. Therefore:</p>
<img alt="\[\frac{\partial Q}{\partial \theta_1}=2\theta_1X^{2}_{11}+2\theta_2X^{2}_{12}+...+2\theta_nX^{2}_{1n}\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/832b294f472a23e500616db08d9d6832770af6a3.png" style="height: 40px;">
<p>The partial derivatives by other <img alt="\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> components are similar.
Collecting the sequence of partial derivatives back into a vector equation, we
get:</p>
<img alt="\[\frac{\partial Q}{\partial \theta}=2X^2\theta=2X^TX\theta\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/541124d49fa78dcf92a15b14643b2ebc4187eaaf.png" style="height: 38px;">
<p>Now back to <em>J</em>. Recall that for convenience we broke <em>J</em> up into three parts:
<em>P</em>, <em>Q</em> and <img alt="y^Ty" class="valign-m4" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/81015d6225923cec985bef47ca151ef1cb654c92.png" style="height: 19px;">; the latter doesn't depend on <img alt="\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> so it
doesn't play a role in the derivative. Collecting our results from this post, we
then get:</p>
<img alt="\[\frac{\partial J}{\partial \theta}=\frac{\partial Q}{\partial \theta}-\frac{\partial P}{\partial \theta}=2X^TX\theta-2X^Ty\]" class="align-center" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/9c3d0d108ada3bfc7290c2328c8e6171bc01d7de.png" style="height: 38px;">
<p>Which is exactly the equation we were expecting to see.</p>
<p>To conclude - if matrix calculus feels handwavy, it's because its identities are
less familiar. In a sense, it's handwavy in the same way
<img alt="\frac{dx^2}{dx}=2x" class="valign-m6" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/5fa725ae5b10a9249e9480d595770cf34accf533.png" style="height: 24px;"> is handwavy. We remember the identity so we don't
have to recalculate it every time from first principles. Once you get some
experience with matrix calculus, parts of equations start looking familiar and
you no longer need to engage in the long and tiresome computations demonstrated
here. It's perfectly fine to just remember that the derivative of
<img alt="\theta^TX\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/7616542d90e084c74423b2a9d93b7a3a6cadcd00.png" style="height: 15px;"> with a symmetric <strong>X</strong> is <img alt="2X\theta" class="valign-0" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/7fa6bcc17eae56f6f3f4a6fdcadae3cb3ee2c5d7.png" style="height: 12px;">. See the
"identities" section of the <a class="reference external" href="http://en.wikipedia.org/wiki/Matrix_calculus">wikipedia article on matrix calculus</a> for many more examples.</p>
<hr class="docutils">
<table class="docutils footnote" frame="void" id="id4" rules="none">
<colgroup><col class="label"><col></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id1">[1]</a></td><td>A few words on notation: by default, a vector <strong>v</strong> is a <em>column</em> vector.
To get its row version, we transpose it. Moreover, in the vector
derivative equations that follow I'm using <a class="reference external" href="http://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions">denominator layout notation</a>. This
is not super-important though; as the Wikipedia article suggests, many
mathematical papers and writings aren't consistent about this and it's
perfectly possible to understand the derivations regardless.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id5" rules="none">
<colgroup><col class="label"><col></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id2">[2]</a></td><td>Yes, this is exactly like computing a gradient of a multivariate
function.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label"><col></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#id3">[3]</a></td><td>Take a minute to convince yourself that the dimensions of this equation
work out and the result is a scalar.</td></tr>
</tbody>
</table>

            </div>
            <!-- /.entry-content -->
<hr>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>,
or reach out <a href="https://twitter.com/elibendersky">on Twitter</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            © 2003-2019 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script async="" src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/analytics.js"></script><script src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/jquery-2.1.1.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="./The Normal Equation and matrix calculus - Eli Bendersky&#39;s website_files/respond.min.js"></script>

<script type="text/javascript">

  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-54426790-1', 'auto');
  ga('send', 'pageview');

</script>

</body></html>