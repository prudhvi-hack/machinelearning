<!DOCTYPE html>
<!-- saved from url=(0091)https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:og="http://ogp.me/ns#" xmlns:fb="https://www.facebook.com/2008/fbml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Derivation of the Normal Equation for linear regression - Eli Bendersky's website</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <link href="https://eli.thegreenplace.net/favicon.ico" rel="icon">

    <!-- Bootstrap -->
        <link rel="stylesheet" href="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/bootstrap.min.css" type="text/css">
    <link href="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/font-awesome.min.css" rel="stylesheet">

    <link href="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/vs.css" rel="stylesheet">
    <link rel="stylesheet" href="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/style.css" type="text/css">

        <link href="https://eli.thegreenplace.net/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Eli Bendersky&#39;s website ATOM Feed">

</head>
<body>

<div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="https://eli.thegreenplace.net/" class="navbar-brand">
                <img src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/logosmall.png" width="32">
Eli Bendersky's website            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav navbar-right">
                <li>
                    <a href="https://eli.thegreenplace.net/pages/about">
                        <i class="fa fa-question"></i>
                        <span class="icon-label">About</span>
                    </a>
                </li>
                <li>
                    <a href="https://eli.thegreenplace.net/archives/all">
                        <i class="fa fa-th-list"></i>
                        <span class="icon-label">Archives</span>
                    </a>
                </li>
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<div class="container">
    <div class="row">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/" rel="bookmark" title="Permalink to Derivation of the Normal Equation for linear regression">
                        Derivation of the Normal Equation for linear regression
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="published">
        <i class="fa fa-calendar"></i>
        <time> December 22, 2014 at 20:50</time>
    </span>
<span class="label label-default">Tags</span>
    <a href="https://eli.thegreenplace.net/tag/math">Math</a>
        ,
    <a href="https://eli.thegreenplace.net/tag/machine-learning">Machine Learning</a>
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>I was going through the Coursera "Machine Learning" course, and in the section
on multivariate linear regression something caught my eye. Andrew Ng presented
the <a class="reference external" href="http://en.wikipedia.org/w/index.php?title=Normal_equation&amp;redirect=no">Normal Equation</a> as an
analytical solution to the linear regression problem with a least-squares cost
function. He mentioned that in some cases (such as for small feature sets) using
it is more effective than applying gradient descent; unfortunately, he left its
derivation out.</p>
<p>Here I want to show how the normal equation is derived.</p>
<p>First, some terminology. The following symbols are compatible with the machine
learning course, not with the exposition of the normal equation on Wikipedia and
other sites - semantically it's all the same, just the symbols are different.</p>
<p>Given the hypothesis function:</p>
<img alt="\[h_{\theta}(x)=\theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/dd8fad9bf111e83d47252d51dd037a6c6c3136aa.png" style="height: 18px;">
<p>We'd like to minimize the least-squares cost:</p>
<img alt="\[J(\theta_{0...n})=\frac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/c1abe0768f4deb31ed97f37d760236c94439a780.png" style="height: 50px;">
<p>Where <img alt="x^{(i)}" class="valign-0" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/233014006c0adbee71ec71ba3a70f22ad1b906a1.png" style="height: 17px;"> is the <tt class="docutils literal">i</tt>-th sample (from a set of <tt class="docutils literal">m</tt> samples) and
<img alt="y^{(i)}" class="valign-m4" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/d34414117d493106f731939df6bb7f1762365d3f.png" style="height: 21px;"> is the <tt class="docutils literal">i</tt>-th expected result.</p>
<p>To proceed, we'll represent the problem in matrix notation; this is natural,
since we essentially have a system of linear equations here. The regression
coefficients <img alt="\theta" class="valign-0" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> we're looking for are the vector:</p>
<img alt="\[\begin{pmatrix} \theta_0\\ \theta_1\\ ...\\ \theta_n \end{pmatrix}\in\mathbb{R}^{n+1}\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/b16fd3d2b3041f13cb70199837a7c02c756078c7.png" style="height: 86px;">
<p>Each of the <tt class="docutils literal">m</tt> input samples is similarly a column vector with <tt class="docutils literal">n+1</tt> rows,
<img alt="x_0" class="valign-m3" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/efbda784ad565c1c5201fdc948a570d0426bc6e6.png" style="height: 11px;"> being 1 for convenience. So we can now rewrite the hypothesis
function as:</p>
<img alt="\[h_{\theta}(x)=\theta^Tx\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/be661047c89f6a48c7bc563b81207949c251de6a.png" style="height: 21px;">
<p>When this is summed over all samples, we can dip further into matrix notation.
We'll define the "design matrix" <tt class="docutils literal">X</tt> (uppercase X) as a matrix of <tt class="docutils literal">m</tt> rows,
in which each row is the <tt class="docutils literal">i</tt>-th sample (the vector <img alt="x^{(i)}" class="valign-0" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/233014006c0adbee71ec71ba3a70f22ad1b906a1.png" style="height: 17px;">). With
this, we can rewrite the least-squares cost as following, replacing the explicit
sum by matrix multiplication:</p>
<img alt="\[J(\theta)=\frac{1}{2m}(X\theta-y)^T(X\theta-y)\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/db5e3da78e25c18c8fc88f1291c1ac13a2645388.png" style="height: 36px;">
<p>Now, using some matrix transpose identities, we can simplify this a bit. I'll
throw the <img alt="\frac{1}{2m}" class="valign-m6" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/7a2a3f6dba54b64f0e88e18c40e0f68c523713ea.png" style="height: 22px;"> part away since we're going to compare a
derivative to zero anyway:</p>
<img alt="\[J(\theta)=((X\theta)^T-y^T)(X\theta-y)\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/c1368de1a0634c3fbeb92d67f368f253943d089f.png" style="height: 21px;">
<img alt="\[J(\theta)=(X\theta)^TX\theta-(X\theta)^Ty-y^T(X\theta)+y^Ty\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/e41fc822adccf1f865b02100f5671e265e7b30bc.png" style="height: 21px;">
<p>Note that <img alt="X\theta" class="valign-0" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/52f2de6065bdc187b876c5696041f3c716c446f5.png" style="height: 12px;"> is a vector, and so is <tt class="docutils literal">y</tt>. So when we multiply
one by another, it doesn't matter what the order is (as long as the dimensions
work out). So we can further simplify:</p>
<img alt="\[J(\theta)=\theta^TX^TX\theta-2(X\theta)^Ty+y^Ty\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/2864b88546c007a79dc92271f5e01487ba608e43.png" style="height: 21px;">
<p>Recall that here <img alt="\theta" class="valign-0" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> is our unknown. To find where the above
function has a minimum, we will derive by <img alt="\theta" class="valign-0" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/cb005d76f9f2e394a770c2562c2e150a413b3216.png" style="height: 12px;"> and compare to 0.
Deriving by a vector may feel uncomfortable, but there's nothing to worry about.
Recall that here we only use matrix notation to conveniently represent a system
of linear formulae. So we derive by each component of the vector, and then
combine the resulting derivatives into a vector again. The result is:</p>
<img alt="\[\frac{\partial J}{\partial \theta}=2X^TX\theta-2X^{T}y=0\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/9b142c00e031c9db7f575b0542e86261732a4689.png" style="height: 38px;">
<p>Or:</p>
<img alt="\[X^TX\theta=X^{T}y\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/ab453f9f1f7bd4b1d646b9712fbe0b2fbe01740f.png" style="height: 21px;">
<p>Now, assuming that the matrix <img alt="X^TX" class="valign-0" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/5c817c84ec1f83b23494df6125edd091a7c413dd.png" style="height: 15px;"> is invertible, we can multiply both
sides by <img alt="(X^TX)^{-1}" class="valign-m4" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/57f592cee6ceac659262d97e61c64f9ca405d7f1.png" style="height: 19px;"> and get:</p>
<img alt="\[\theta=(X^TX)^{-1}X^Ty\]" class="align-center" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/20baabd9d33dcd26003bc44c7d81ba39e1ad4caa.png" style="height: 21px;">
<p>Which is the normal equation.</p>
<p>[<strong>Update 27-May-2015</strong>: I've written <a class="reference external" href="http://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/">another post</a>
that explains in more detail how these derivatives are computed.]</p>

            </div>
            <!-- /.entry-content -->
<hr>
<div class="dotted-links">
<p class="align-center">
For comments, please send me
<a href="mailto:eliben@gmail.com"><i class="fa fa-envelope-o"></i> an email</a>,
or reach out <a href="https://twitter.com/elibendersky">on Twitter</a>.
</p>
</div>        </article>
    </section>

    </div>
</div>
<footer>
   <div class="container">
      <hr>
      <div class="row">
         <div class="col-xs-10">
            © 2003-2019 Eli Bendersky
         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression/#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script async="" src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/analytics.js"></script><script src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/jquery-2.1.1.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="./Derivation of the Normal Equation for linear regression - Eli Bendersky&#39;s website_files/respond.min.js"></script>

<script type="text/javascript">

  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-54426790-1', 'auto');
  ga('send', 'pageview');

</script>

</body></html>